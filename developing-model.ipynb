{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Punctation Restoration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Union, List, Optional, Literal, Dict, Iterator, Iterable, Tuple\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import logging\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, PreTrainedTokenizer\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from hydra import compose, initialize_config_module, initialize\n",
    "from omegaconf import DictConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "ZeroClass = Literal['O']\n",
    "PunctuationMark = Literal['.', ',', '?', '!', '-', '...']\n",
    "Label = Union[ZeroClass, PunctuationMark]\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DatasetItem:\n",
    "    first_token_pos: List[int]\n",
    "    token_ids: List[int]\n",
    "    label_ids: List[int]\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert len(self.token_ids) == len(self.label_ids), \\\n",
    "            'the number of tokens does not match the number of labels'\n",
    "        assert len(self.first_token_pos) < len(self.token_ids), \\\n",
    "            'there cannot be more words, than subtokens'\n",
    "        assert all(pos < len(self.token_ids) for pos in self.first_token_pos), \\\n",
    "            'word\\'s first token position cannot exceed the number of subtokens'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "class PunctuationRestorationDataModule(pl.LightningDataModule):\n",
    "    PUNCTUATION_MARKS = ['.', ',', '?', '!', '-', '...']\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: DictConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.tokenizer_name: str = cfg.model.encoder.tokenizer\n",
    "        self.tokenizer: Optional[PreTrainedTokenizer] = None\n",
    "\n",
    "        self.train_batch_size: int = cfg.trainer.train_batch_size\n",
    "        self.val_batch_size: int = cfg.trainer.val_batch_size\n",
    "        self.test_batch_size: int = cfg.trainer.test_batch_size\n",
    "        self.predict_batch_size: int = cfg.trainer.predict_batch_size\n",
    "\n",
    "        self.train_dataset: Optional[Iterable[DatasetItem]] = None\n",
    "        self.val_dataset: Optional[Iterable[DatasetItem]] = None\n",
    "        self.test_dataset: Optional[Iterable[DatasetItem]] = None\n",
    "        self.predict_dataset: Optional[Iterable[DatasetItem]] = None\n",
    "\n",
    "        self.idx2label: List[Label] = ['O'] + self.PUNCTUATION_MARKS\n",
    "        self.label2idx: Dict[Label, int] = {label: idx for idx, label in enumerate(self.idx2label)}\n",
    "        self.num_classes: int = len(self.idx2label)\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "\n",
    "    def _parse_tsv_input(self, filepath: Path) -> Iterable[DatasetItem]:\n",
    "        pass\n",
    "\n",
    "    # TODO max_seq_len\n",
    "    def _parse_json_input(self, filepath: Path) -> Iterable[DatasetItem]:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        items: List[DatasetItem] = []\n",
    "        for document in data:\n",
    "            first_token_pos: List[int] = []\n",
    "            token_ids: List[int] = []\n",
    "            label_ids: List[int] = []\n",
    "            for word in document['words']:\n",
    "                if len(token_ids) >= self.cfg.trainer.max_seq_len-2:\n",
    "                    break\n",
    "\n",
    "                first_token_pos.append(len(token_ids))\n",
    "\n",
    "                subtokens = self.tokenizer.tokenize(word['word'], )\n",
    "                token_ids.extend(self.tokenizer.convert_tokens_to_ids(subtokens))\n",
    "\n",
    "                label_ids.extend([self.label2idx.get(word['punctuation'], 0)] + [0] * (len(subtokens) - 1))\n",
    "\n",
    "            token_ids = [self.tokenizer.cls_token_id] + token_ids[:self.cfg.trainer.max_seq_len-2] + [self.tokenizer.sep_token_id]\n",
    "            label_ids = [0] + label_ids[:self.cfg.trainer.max_seq_len-2] + [0]\n",
    "\n",
    "            items.append(DatasetItem(first_token_pos, token_ids, label_ids))\n",
    "\n",
    "        return items\n",
    "\n",
    "    # TODO other formats?\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "\n",
    "        if stage is None or stage == 'train':\n",
    "            if self.cfg.data.train.endswith('.json'):\n",
    "                self.train_dataset = self._parse_json_input(self.cfg.data.train)\n",
    "            else:\n",
    "                raise NotImplementedError('unknown format')\n",
    "\n",
    "        if stage is None or stage == 'val':\n",
    "            if self.cfg.data.val.endswith('.json'):\n",
    "                self.val_dataset = self._parse_json_input(self.cfg.data.val)\n",
    "            else:\n",
    "                raise NotImplementedError('unknown format')\n",
    "\n",
    "        if stage is None or stage == 'test':\n",
    "            if self.cfg.data.test.endswith('.json'):\n",
    "                self.test_dataset = self._parse_json_input(self.cfg.data.test)\n",
    "            else:\n",
    "                raise NotImplementedError('unknown format')\n",
    "\n",
    "        if stage is None or stage == 'predict':\n",
    "            if self.cfg.data.predict.endswith('.json'):\n",
    "                self.predict_dataset = self._parse_json_input(self.cfg.data.predict)\n",
    "            else:\n",
    "                raise NotImplementedError('unknown format')\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(self.train_dataset,\n",
    "                          batch_size=self.train_batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=self.collator)\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.val_dataset,\n",
    "                          batch_size=self.val_batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=self.collator)\n",
    "\n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.test_dataset,\n",
    "                          batch_size=self.test_batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=self.collator)\n",
    "\n",
    "    def predict_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.predict_dataset,\n",
    "                          batch_size=self.predict_batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=self.collator)\n",
    "\n",
    "    def collator(self, batch: Iterable[DatasetItem]) -> Tuple[List[List[int]], List[torch.Tensor], List[torch.Tensor]]:\n",
    "        first_token_pos = [item.first_token_pos for item in batch]\n",
    "        token_ids = [item.token_ids for item in batch]\n",
    "        label_ids = [torch.tensor(item.label_ids) for item in batch]\n",
    "\n",
    "        padded_token_ids = [self.tokenizer.pad(\n",
    "            {'input_ids': tokens},\n",
    "            padding='longest',\n",
    "            return_tensors='pt'\n",
    "        ) for tokens in token_ids]\n",
    "\n",
    "        return first_token_pos, padded_token_ids, label_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "configs  dataloader.py\t\t main.py   outputs\r\n",
      "data\t developing-model.ipynb  model.py  scripts\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 8, 9, 10, 12, 14, 15, 16, 19, 22, 23, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 42, 44, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 62, 63, 64, 65, 66, 67, 68, 71, 72, 73, 74, 75, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125]\n",
      "\n",
      "{'input_ids': tensor([    0,  2181, 10347,  7990,  1055,  2289,    76,  6425,  4323,  4714,\n",
      "         2282,  4737,  2178,  7418,  5657,  6105, 13087, 25528,  3453,  1024,\n",
      "         2003,    93, 17861,  4506,  2003,    93,  2572, 16425,  2040,  5060,\n",
      "        42135,  2642, 26168,  2408, 49699,  9791,  9843,  1019,  2194,  2254,\n",
      "        11938,  2181, 10347,  7990,  1055,  2289,    76,  6425,  4323, 13944,\n",
      "        37028, 35023, 13376, 43704,  5874, 12593, 34122,  1029,  1019,  2289,\n",
      "           76,  6425,  1997, 14324,  2040, 27743, 19801,  1998, 12093, 25528,\n",
      "         3453,  1024,  5114,  2022,  2408, 49699,  9791,  9843, 10387,  8905,\n",
      "        13789, 12593,  1046,  3131,  2431,  2291,  1998,  2553,  2291,  3420,\n",
      "        20488,  1998,  6530,  3723, 25062, 29592,  2944, 12462,  1009,  3938,\n",
      "        11938, 14324,  2040, 13171, 31267,  1998,  2194, 12093,  9460,  2022,\n",
      "         1019,  5235,  6105, 13087, 15922,  2040,  6012, 15895, 12593, 32710,\n",
      "         2413,  1998,  9302, 11351,  8112,  2634,  3695,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 2, 0, 0, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with initialize(version_base=None, config_path='configs/'):\n",
    "    cfg = compose(config_name='config.yaml')\n",
    "    datamodule = PunctuationRestorationDataModule(cfg)\n",
    "    datamodule.setup()\n",
    "\n",
    "    dataloader = datamodule.test_dataloader()\n",
    "\n",
    "    for item in dataloader:\n",
    "        first_token_pos, token_ids, label_ids = item\n",
    "\n",
    "        print(first_token_pos[0], end='\\n\\n')\n",
    "        print(token_ids[0], end='\\n\\n')\n",
    "        print(label_ids[0], end='\\n\\n')\n",
    "\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class RestorationModel(pl.LightningModule):\n",
    "    def __init__(self, cfg: DictConfig, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.save_hyperparameters(cfg)\n",
    "\n",
    "        self.encoder = AutoModel.from_pretrained(cfg.model.encoder.name)\n",
    "        self.head = self._construct_head()\n",
    "\n",
    "    def _construct_head(self) -> nn.Module:\n",
    "        if self.hparams.model.head.architecture == 'mlp':\n",
    "            mlp_config = self.hparams.model.head.mlp\n",
    "\n",
    "            head = nn.Sequential()\n",
    "            prev_layer_dim = self.encoder.config.hidden_size\n",
    "            for i in range(mlp_config.num_layers - 1):\n",
    "                layer = nn.Linear(prev_layer_dim, mlp_config.num_hiddens)\n",
    "                prev_layer_dim = layer.out_features\n",
    "\n",
    "                if mlp_config.nonlinearity == 'relu':\n",
    "                    nonlinearity = nn.ReLU()\n",
    "                else:\n",
    "                    raise ValueError('unknown nonlinearity')\n",
    "\n",
    "                head.add_module(f'cls{i}', layer)\n",
    "                head.add_module(f'cls{i}_nonlinearity', nonlinearity)\n",
    "\n",
    "            head.add_module('cls_pred', nn.Linear(prev_layer_dim, self.num_classes))\n",
    "\n",
    "            return head\n",
    "\n",
    "        else:\n",
    "            raise ValueError('unknown head architecture')\n",
    "\n",
    "    def common_step(self, batch):\n",
    "        first_token_pos, model_inputs_batch, label_ids_batch = batch\n",
    "\n",
    "        batch_preds = []\n",
    "        batch_golds = []\n",
    "\n",
    "        for model_inputs, label_ids in zip(model_inputs_batch, label_ids_batch):\n",
    "            encoded = self.encoder(\n",
    "                input_ids=model_inputs['input_ids'].unsqueeze(0),\n",
    "                attention_mask=model_inputs['attention_mask'].unsqueeze(0),\n",
    "                return_dict=True\n",
    "            )['last_hidden_state'].squeeze()\n",
    "\n",
    "            logits = self.head(encoded)\n",
    "\n",
    "            batch_preds.append(logits)\n",
    "            batch_golds.append(label_ids)\n",
    "\n",
    "        batch_preds_tensor = torch.tensor(batch_preds)\n",
    "        batch_golds_tensor = torch.tensor(batch_golds)\n",
    "\n",
    "        loss = F.cross_entropy(batch_preds_tensor.view(-1, batch_preds_tensor.size(-1)),\n",
    "                               batch_golds_tensor.view(-1))\n",
    "\n",
    "        return batch_preds, batch_golds, loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        preds, golds, loss = self.common_step(batch)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        preds, golds, loss = self.common_step(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        return preds, golds\n",
    "\n",
    "    # FIXME shouldn't it be validation_step_end instead? https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#validating-with-dataparallel\n",
    "    def validation_epoch_end(self, validation_step_outputs) -> None:\n",
    "        print(validation_step_outputs)\n",
    "\n",
    "        # TODO softmax\n",
    "        all_preds = torch.stack(validation_step_outputs)\n",
    "\n",
    "        # TODO calculate and log metrics\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        preds, golds, loss = self.common_step(batch)\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.training.learning_rate)\n",
    "        return optimizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "model_inputs = {'input_ids': torch.tensor([    0,  2181, 10347,  7990,  1055,  2289,    76,  6425,  4323,  4714,\n",
    "                                      2282,  4737,  2178,  7418,  5657,  6105, 13087, 25528,  3453,  1024,\n",
    "                                      2003,    93, 17861,  4506,  2003,    93,  2572, 16425,  2040,  5060,\n",
    "                                      42135,  2642, 26168,  2408, 49699,  9791,  9843,  1019,  2194,  2254,\n",
    "                                      11938,  2181, 10347,  7990,  1055,  2289,    76,  6425,  4323, 13944,\n",
    "                                      37028, 35023, 13376, 43704,  5874, 12593, 34122,  1029,  1019,  2289,\n",
    "                                      76,  6425,  1997, 14324,  2040, 27743, 19801,  1998, 12093, 25528,\n",
    "                                      3453,  1024,  5114,  2022,  2408, 49699,  9791,  9843, 10387,  8905,\n",
    "                                      13789, 12593,  1046,  3131,  2431,  2291,  1998,  2553,  2291,  3420,\n",
    "                                      20488,  1998,  6530,  3723, 25062, 29592,  2944, 12462,  1009,  3938,\n",
    "                                      11938, 14324,  2040, 13171, 31267,  1998,  2194, 12093,  9460,  2022,\n",
    "                                      1019,  5235,  6105, 13087, 15922,  2040,  6012, 15895, 12593, 32710,\n",
    "                                      2413,  1998,  9302, 11351,  8112,  2634,  3695,     2]), 'attention_mask': torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "                                                                                                                         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "                                                                                                                         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "                                                                                                                         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "                                                                                                                         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "                                                                                                                         1, 1, 1, 1, 1, 1, 1, 1])}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/624M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd141c90df064b538e429c5fa663fe0f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.sso.sso_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "encoder = AutoModel.from_pretrained('allegro/herbert-base-cased')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "token_ids = token_ids.unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "r = encoder(token_ids[0]['input_ids'].unsqueeze(0), return_dict=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "odict_keys(['last_hidden_state', 'pooler_output'])"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 128, 768]), torch.Size([1, 768]))"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['last_hidden_state'].size(), r['pooler_output'].size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 128, 768]), torch.Size([1, 768]))"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0].shape, r[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.0703, -0.0108, -0.0752,  ..., -0.7747,  0.0734,  0.2588],\n         [-0.3318,  0.2007, -0.4598,  ...,  0.6457,  0.1525,  0.0627],\n         [ 0.3860,  0.2852, -0.7873,  ..., -1.2532,  0.2973,  1.0523],\n         ...,\n         [ 0.1820, -0.0097, -0.1943,  ..., -0.4040,  0.1927, -0.1603],\n         [ 0.2354, -0.1440, -0.0039,  ..., -0.9941, -0.1513, -0.6591],\n         [ 0.3854,  0.2836, -0.7765,  ..., -1.2490,  0.3054,  1.0535]]],\n       grad_fn=<NativeLayerNormBackward>)"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "transformers.models.bert.modeling_bert.BertModel"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([128, 768])"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['last_hidden_state'].squeeze().size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "768"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['last_hidden_state'].size(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([98304])"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['last_hidden_state'].view(-1).size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "main",
   "language": "python",
   "display_name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}